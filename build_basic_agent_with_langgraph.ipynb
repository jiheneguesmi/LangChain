{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWz0RJuuINYu"
      },
      "source": [
        "LangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\n",
        "\n",
        "Reliability and controllability. Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\n",
        "Low-level and extensible. Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\n",
        "First-class streaming support. With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeyAjAfHF1U_",
        "outputId": "ce8d806d-67b9-4da0-c63c-c2ac51f37e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.4.8)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.45)\n",
            "Collecting langsmith\n",
            "  Using cached langsmith-0.4.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.66)\n",
            "Requirement already satisfied: langgraph-checkpoint>=2.0.26 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.1.0)\n",
            "Requirement already satisfied: langgraph-prebuilt>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.2.2)\n",
            "Requirement already satisfied: langgraph-sdk>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.70)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.18)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.14.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint>=2.0.26->langgraph) (1.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (2.4.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
            "Downloading langsmith-0.4.1-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.6/364.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langsmith\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.45\n",
            "    Uninstalling langsmith-0.3.45:\n",
            "      Successfully uninstalled langsmith-0.3.45\n",
            "Successfully installed langsmith-0.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langgraph langsmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lusZqGL3Ng7j"
      },
      "source": [
        "# 2. Create a StateGraph¶\n",
        "Now you can create a basic chatbot using LangGraph. This chatbot will respond directly to user messages.\n",
        "\n",
        "Start by creating a StateGraph. A StateGraph object defines the structure of our chatbot as a \"state machine\". We'll add nodes to represent the llm and functions our chatbot can call and edges to specify how the bot should transition between these functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cyQ2ig6SNb3x"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph, START\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    # Messages have the type \"list\". The `add_messages` function\n",
        "    # in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOaomnbrQZJm"
      },
      "source": [
        "Our graph can now handle two key tasks:\n",
        "\n",
        "Each node can receive the current State as input and output an update to the state.\n",
        "Updates to messages will be appended to the existing list rather than overwriting it, thanks to the prebuilt add_messages function used with the Annotated syntax.\n",
        "# 3. Add a node\n",
        "Next, add a \"chatbot\" node. Nodes represent units of work and are typically regular Python functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7c2RfhOQsTU",
        "outputId": "119be747-c8f6-4d8c-ac9c-2ca03dd34719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain[google-genai] in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (0.3.66)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (0.4.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (6.0.2)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.1.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain[google-genai]) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain[google-genai]) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain[google-genai]) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain[google-genai]) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain[google-genai]) (3.2.3)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai->langchain[google-genai]) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai->langchain[google-genai]) (0.6.18)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (5.29.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[google-genai]) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[google-genai]) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[google-genai]) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (3.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (4.9.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[google-genai]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \"langchain[google-genai]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x5SoGSEPrP8D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAcAN07JdVEfiST3OXvn7uUa9o8oxtwccg\"\n",
        "\n",
        "llm = init_chat_model(\"google_genai:gemini-2.0-flash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evqv3W0nswa8",
        "outputId": "a2a3cb16-0ab1-497e-d6e0-f419c1d6a1ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7ccc008ad090>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "# The first argument is the unique node name\n",
        "# The second argument is the function or object that will be called whenever\n",
        "# the node is used.\n",
        "graph_builder.add_node(\"chatbot\", chatbot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24pUXXH24b3d"
      },
      "source": [
        "the chatbot node function takes the current State as input and returns a dictionary containing an updated messages list under the key \"messages\". This is the basic pattern for all LangGraph node functions.\n",
        "\n",
        "The add_messages function in our State will append the LLM's response messages to whatever messages are already in the state.\n",
        "# 4. Add an entry point\n",
        "Add an entry point to tell the graph where to start its work each time it is run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioqvWLMAsxES",
        "outputId": "d6f36247-7444-418e-c60c-d1979fd12835"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7ccc008ad090>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "graph_builder.add_edge(START, \"chatbot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAshQhdS6sLV"
      },
      "source": [
        "# 5. Compile the graph¶\n",
        "Before running the graph, we'll need to compile it. We can do so by calling compile() on the graph builder. This creates a CompiledGraph we can invoke on our state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YcRd3PO36o_V"
      },
      "outputs": [],
      "source": [
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnyVgins7WCc"
      },
      "source": [
        "# 6. Visualize the graph\n",
        "You can visualize the graph using the get_graph method and one of the \"draw\" methods, like draw_ascii or draw_png. The draw methods each require additional dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "iZTqdiYQ6-tg",
        "outputId": "deaa7c05-3a45-48d5-a061-7dba58428d05"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXkgWwMl8Lta"
      },
      "source": [
        "7. Run the chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48KtRMNH8GMD",
        "outputId": "19556e01-9c25-41ce-8487-12e295a33470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: how the weather today in tunisia\n",
            "Assistant: I can't give you a real-time, up-to-the-minute weather report for Tunisia. Weather conditions change very rapidly.\n",
            "\n",
            "However, here's how you can get the most accurate and current information:\n",
            "\n",
            "*   **Check a reliable weather app or website:** Some popular options include:\n",
            "    *   AccuWeather\n",
            "    *   The Weather Channel\n",
            "    *   Google Weather (just type \"weather in [city in Tunisia]\" into the Google search bar)\n",
            "    *   MeteoTunisie (the official Tunisian meteorological service)\n",
            "*   **Specify a city:** Tunisia is a country, and the weather can vary quite a bit from north to south, and coast to inland. When you search, specify a city like Tunis, Sfax, Sousse, Djerba, etc.\n",
            "\n",
            "When you check, you'll likely find information about:\n",
            "\n",
            "*   Temperature (high and low)\n",
            "*   Conditions (sunny, cloudy, rainy, etc.)\n",
            "*   Wind speed and direction\n",
            "*   Humidity\n",
            "*   Chance of precipitation\n",
            "\n",
            "Enjoy your day in Tunisia!\n",
            "User: What do you know about LangGraph?\n",
            "Assistant: LangGraph is a powerful framework developed by LangChain for building **stateful, multi-actor conversational applications**. It's designed to help you create complex, dynamic workflows that go beyond simple linear conversations. Think of it as a way to orchestrate a complex dance between different agents, tools, and data sources, all guided by a defined state.\n",
            "\n",
            "Here's a breakdown of what I know about LangGraph:\n",
            "\n",
            "**Key Concepts:**\n",
            "\n",
            "*   **Nodes:** These are the fundamental building blocks of your graph. A node can be anything that processes information and transforms the state. Common node types include:\n",
            "    *   **Agents:** Agents use language models to make decisions and take actions.  They can leverage tools, query databases, or interact with external APIs.\n",
            "    *   **Functions:** Python functions that perform specific tasks, like data retrieval or transformation.\n",
            "    *   **Conditionals:** Nodes that decide which path to take based on the current state.\n",
            "    *   **LLM Chains:** Pre-built sequences of prompts, models, and parsers for specific tasks.\n",
            "*   **Edges:** Edges define the connections between nodes, dictating the flow of information and control.  They can be:\n",
            "    *   **Directed Edges:**  Represent a simple transition from one node to another.\n",
            "    *   **Conditional Edges:** Determine the next node to visit based on a condition. This allows for branching logic.\n",
            "*   **State:** The state represents the current state of the conversation or workflow. It's a shared memory that all nodes can access and modify. The state is typically a dictionary or dataclass containing relevant information, such as user input, agent decisions, retrieved data, and intermediate results.\n",
            "*   **Graph:** The graph is the central structure that connects nodes and edges, defining the overall workflow.  It manages the execution of the nodes and the transitions between them.\n",
            "*   **Compiler (LCEL - LangChain Expression Language):** LangGraph leverages LCEL, a declarative way to define the graph's structure and execution flow.  LCEL makes it easy to define the flow of data and control through the graph.\n",
            "\n",
            "**Key Features and Benefits:**\n",
            "\n",
            "*   **State Management:**  Centralized state management allows nodes to share information and maintain context throughout the conversation. This is crucial for complex, multi-turn interactions.\n",
            "*   **Multi-Agent Collaboration:**  LangGraph enables you to create applications where multiple agents work together, each with its own role and responsibilities.\n",
            "*   **Dynamic Workflows:**  Conditional edges allow you to create workflows that adapt to the user's input and the results of previous actions.\n",
            "*   **Modularity and Reusability:**  Nodes can be reused in different graphs, promoting code reuse and simplifying development.\n",
            "*   **Observability and Debugging:**  LangGraph provides tools for visualizing and debugging the execution of your graph, making it easier to understand and troubleshoot complex workflows.\n",
            "*   **Integration with LangChain:** LangGraph seamlessly integrates with other LangChain components, such as LLMs, tools, and memory.\n",
            "*   **Scalability:** Designed to handle complex applications with many agents and nodes.\n",
            "*   **Flexibility:** Can be used to build a wide range of applications, from conversational agents to automated workflows.\n",
            "\n",
            "**Use Cases:**\n",
            "\n",
            "*   **Complex Conversational Agents:** Build agents that can handle complex tasks, such as booking travel, providing customer support, or conducting research.\n",
            "*   **Automated Workflows:** Automate tasks that involve multiple steps and decisions, such as loan applications, document processing, or software development.\n",
            "*   **Multi-Agent Systems:** Create systems where multiple agents collaborate to solve a problem, such as a team of agents working together to design a new product.\n",
            "*   **Research and Experimentation:**  Quickly prototype and experiment with different agent architectures and workflows.\n",
            "\n",
            "**How it Differs from LangChain Chains:**\n",
            "\n",
            "While LangChain chains are great for linear sequences of operations, LangGraph excels at handling more complex, non-linear workflows.  Think of it this way:\n",
            "\n",
            "*   **LangChain Chains:**  A pipeline for processing data in a sequential manner.\n",
            "*   **LangGraph:** A graph-based structure that allows for branching, looping, and state management, enabling more sophisticated and dynamic interactions.\n",
            "\n",
            "**Example Scenario:**\n",
            "\n",
            "Imagine building a customer service agent.\n",
            "\n",
            "*   **Node 1 (User Input):** Captures the user's question.\n",
            "*   **Node 2 (Intent Classifier Agent):** Uses an LLM to determine the user's intent (e.g., \"track order,\" \"return item,\" \"ask about product\").\n",
            "*   **Conditional Edge:** Based on the intent, the graph branches to the appropriate node.\n",
            "    *   **If Intent is \"track order\":**\n",
            "        *   **Node 3 (Order Retrieval Function):** Retrieves the order information from a database.\n",
            "        *   **Node 4 (Response Generation Agent):** Uses an LLM to generate a response with the order tracking information.\n",
            "    *   **If Intent is \"return item\":**\n",
            "        *   **Node 5 (Return Policy Agent):** Uses an LLM to access the return policy and determine eligibility.\n",
            "        *   **Conditional Edge:** Branches based on eligibility.\n",
            "            *   **If Eligible:**\n",
            "                *   **Node 6 (Return Initiation Function):** Initiates the return process.\n",
            "                *   **Node 7 (Confirmation Agent):** Generates a confirmation message.\n",
            "            *   **If Not Eligible:**\n",
            "                *   **Node 8 (Explanation Agent):** Explains why the return is not eligible.\n",
            "\n",
            "The state would track things like the user's order ID, the current stage of the return process, and any error messages.\n",
            "\n",
            "**Getting Started:**\n",
            "\n",
            "To get started with LangGraph, you'll need to:\n",
            "\n",
            "1.  **Install the LangChain library:**  `pip install langchain`\n",
            "2.  **Install the LangGraph library:** `pip install langgraph`\n",
            "3.  **Import the necessary modules:** `from langgraph.graph import StateGraph`\n",
            "4.  **Define your state:** Create a class or dictionary to hold the state of your application.\n",
            "5.  **Create your nodes:** Define the functions, agents, or chains that will process information.\n",
            "6.  **Define your edges:** Connect the nodes with directed or conditional edges.\n",
            "7.  **Build the graph:** Create a `StateGraph` object and add the nodes and edges.\n",
            "8.  **Compile the graph:** Prepare the graph for execution.\n",
            "9.  **Run the graph:** Execute the graph with initial input.\n",
            "\n",
            "**In summary, LangGraph is a powerful and flexible framework for building complex, stateful, multi-agent conversational applications. It provides the tools and abstractions you need to orchestrate complex workflows and create intelligent agents that can handle a wide range of tasks.**\n",
            "\n",
            "To give you a more specific answer, tell me what you're trying to do with LangGraph.  For example:\n",
            "\n",
            "*   \"I'm trying to build a multi-agent system for code review.\"\n",
            "*   \"I want to use LangGraph to automate a document processing workflow.\"\n",
            "*   \"I'm having trouble understanding how to use conditional edges.\"\n",
            "\n",
            "With more context, I can provide more targeted and helpful information.\n"
          ]
        }
      ],
      "source": [
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        # fallback if input() is not available\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v22VPsF-0Xu"
      },
      "source": [
        "# Add tools\n",
        "To handle queries chatbot can't answer \"from memory\", integrate a web search tool. The chatbot can use this tool to find relevant information and provide better responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4wmg-oXm_9gt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c41090-3c89-4e89-8076-1bf360b8a8a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-tavily\n",
            "  Downloading langchain_tavily-0.2.4-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily) (3.11.15)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily) (0.3.26)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily) (0.3.66)\n",
            "Collecting mypy<2.0.0,>=1.15.0 (from langchain-tavily)\n",
            "  Downloading mypy-1.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.20.1)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (0.4.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (4.14.0)\n",
            "Collecting mypy_extensions>=1.0.0 (from mypy<2.0.0,>=1.15.0->langchain-tavily)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from mypy<2.0.0,>=1.15.0->langchain-tavily)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (2025.6.15)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-tavily) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (1.3.1)\n",
            "Downloading langchain_tavily-0.2.4-py3-none-any.whl (24 kB)\n",
            "Downloading mypy-1.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: pathspec, mypy_extensions, mypy, langchain-tavily\n",
            "Successfully installed langchain-tavily-0.2.4 mypy-1.16.1 mypy_extensions-1.1.0 pathspec-0.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-tavily"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxiBft4DABR-"
      },
      "source": [
        "# 2. Configure your environment\n",
        "Configure your environment with search engine API key:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aWvsxNY7_-mM"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EC5r49LAXwn"
      },
      "source": [
        "# 3. Define the tool\n",
        "Define the web search tool:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "W5z-orNCALRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af97e6e-de75-416f-ecb7-67e4e7364ec5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': \"What's a 'node' in LangGraph?\",\n",
              " 'follow_up_questions': None,\n",
              " 'answer': None,\n",
              " 'images': [],\n",
              " 'results': [{'title': 'What is LangGraph? - IBM',\n",
              "   'url': 'https://www.ibm.com/think/topics/langgraph',\n",
              "   'content': 'LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent’s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.',\n",
              "   'score': 0.90907925,\n",
              "   'raw_content': None},\n",
              "  {'title': 'Nodes and Edges | langchain-ai/langgraph-101 | DeepWiki',\n",
              "   'url': 'https://deepwiki.com/langchain-ai/langgraph-101/2.2-nodes-and-edges',\n",
              "   'content': 'Nodes and Edges | langchain-ai/langgraph-101 | DeepWiki Nodes and Edges Nodes and Edges What are Nodes and Edges? In LangGraph, a graph is composed of nodes connected by edges to form a directed workflow. Nodes are the workhorses of LangGraph - they are Python functions that receive the current graph state as input, perform operations, and return updates to that state. Edges define the flow of execution between nodes in a LangGraph. graph_builder.add_edge(\"retrieve_documents\", \"generate_response\") Conditional edges use a function to determine the next node based on the current state. Building a Graph with Nodes and Edges graph_builder.add_node(\"retrieve_documents\", retrieve_documents) graph_builder.add_edge(\"retrieve_documents\", \"generate_response\") When designing nodes and edges in LangGraph: Nodes and Edges What are Nodes and Edges? Building a Graph with Nodes and Edges',\n",
              "   'score': 0.8559598,\n",
              "   'raw_content': None}],\n",
              " 'response_time': 1.18}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from langchain_tavily import TavilySearch\n",
        "\n",
        "tool = TavilySearch(max_results=2)\n",
        "tools = [tool]\n",
        "tool.invoke(\"What's a 'node' in LangGraph?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Define the graph¶\n",
        "This lets the LLM know the correct JSON format to use if it wants to use the search engine."
      ],
      "metadata": {
        "id": "mVJhEMPyrHuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"langchain[google-genai]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NweOTKCdrHPC",
        "outputId": "f4b8386a-0e98-4808-e2a2-b45d4406d55b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain[google-genai] in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (0.3.66)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (0.4.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (6.0.2)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.1.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain[google-genai]) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain[google-genai]) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain[google-genai]) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain[google-genai]) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain[google-genai]) (3.2.3)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai->langchain[google-genai]) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai->langchain[google-genai]) (0.6.18)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (5.29.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[google-genai]) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[google-genai]) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[google-genai]) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain[google-genai]) (3.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (4.9.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain[google-genai]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai->langchain[google-genai]) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAcAN07JdVEfiST3OXvn7uUa9o8oxtwccg\"\n",
        "\n",
        "llm = init_chat_model(\"google_genai:gemini-2.0-flash\")"
      ],
      "metadata": {
        "id": "ulz3WnYYrMHJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Modification: tell the LLM which tools it can call\n",
        "# highlight-next-line\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmgcJcAJrUIe",
        "outputId": "b5ae3b8e-d290-4cfd-e382-97dca2a98c6b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7ccbeec7e250>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Create a function to run the tools\n",
        "Now, we create a function to run the tools if they are called,  by adding the tools to a new node calledBasicToolNode that checks the most recent message in the state and calls tools if the message contains tool_calls. It relies on the LLM's tool_calling support, which is available in Anthropic, OpenAI, Google Gemini, and a number of other LLM providers."
      ],
      "metadata": {
        "id": "jQIUNCBSsvks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "\n",
        "class BasicToolNode:\n",
        "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
        "\n",
        "    def __init__(self, tools: list) -> None:\n",
        "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
        "\n",
        "    def __call__(self, inputs: dict):\n",
        "        if messages := inputs.get(\"messages\", []):\n",
        "            message = messages[-1]\n",
        "        else:\n",
        "            raise ValueError(\"No message found in input\")\n",
        "        outputs = []\n",
        "        for tool_call in message.tool_calls:\n",
        "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
        "                tool_call[\"args\"]\n",
        "            )\n",
        "            outputs.append(\n",
        "                ToolMessage(\n",
        "                    content=json.dumps(tool_result),\n",
        "                    name=tool_call[\"name\"],\n",
        "                    tool_call_id=tool_call[\"id\"],\n",
        "                )\n",
        "            )\n",
        "        return {\"messages\": outputs}\n",
        "\n",
        "\n",
        "tool_node = BasicToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUJi9V5TtJkO",
        "outputId": "382bae40-9757-49d2-dec0-856f2ac15f66"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7ccbeec7e250>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Define the conditional_edges\n",
        "With the tool node added, now we can define the conditional_edges.\n",
        "\n",
        "Edges route the control flow from one node to the next. Conditional edges start from a single node and usually contain \"if\" statements to route to different nodes depending on the current graph state. These functions receive the current graph state and return a string or list of strings indicating which node(s) to call next.\n",
        "\n",
        "Next, we define a router function called route_tools that checks for tool_calls in the chatbot's output. we Provide this function to the graph by calling add_conditional_edges, which tells the graph that whenever the chatbot node completes to check this function to see where to go next.\n",
        "\n",
        "The condition will route to tools if tool calls are present and END if not. Because the condition can return END, you do not need to explicitly set a finish_point this time."
      ],
      "metadata": {
        "id": "nBXg85QdtR2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def route_tools(\n",
        "    state: State,\n",
        "):\n",
        "    \"\"\"\n",
        "    Use in the conditional_edge to route to the ToolNode if the last message\n",
        "    has tool calls. Otherwise, route to the end.\n",
        "    \"\"\"\n",
        "    if isinstance(state, list):\n",
        "        ai_message = state[-1]\n",
        "    elif messages := state.get(\"messages\", []):\n",
        "        ai_message = messages[-1]\n",
        "    else:\n",
        "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
        "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "\n",
        "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if\n",
        "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    route_tools,\n",
        "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
        "    # It defaults to the identity function, but if you\n",
        "    # want to use a node named something else apart from \"tools\",\n",
        "    # You can update the value of the dictionary to something else\n",
        "    # e.g., \"tools\": \"my_tools\"\n",
        "    {\"tools\": \"tools\", END: END},\n",
        ")\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "6V74pVuNtL4S"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Visualize the graph\n",
        "We can visualize the graph using the get_graph method and one of the \"draw\" methods, like draw_ascii or draw_png. The draw methods each require additional dependencies."
      ],
      "metadata": {
        "id": "n-L-PwR6xJDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "PcCD-numxDaX",
        "outputId": "966135f5-2d21-4bdc-dcf9-71b7ff256751"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Ask the bot questions¶\n",
        "Now we can ask the chatbot questions outside its training data:"
      ],
      "metadata": {
        "id": "JNOZJDodxikM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        # fallback if input() is not available\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64kodOQYxWuv",
        "outputId": "0b4454e6-84f8-4d3c-bf56-3bb9c76a58de"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: what is  integrated preparatory cycle in tunisia\n",
            "Assistant: \n",
            "Assistant: {\"query\": \"integrated preparatory cycle in Tunisia\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"The Preparatory Cycle for Engineering Studies in UAS\", \"url\": \"https://uas.ens.tn/en/preparatory-classes/\", \"content\": \"The preparatory cycle also allows students to sit for many examinations to enter engineering schools either in Tunisia or abroad. Moreover, students have the possibility to continue their studies after the preparatory cycle at our university in the engineering specialty of their choice. The preparatory classes offer 4 specializations: Math-Physics\", \"score\": 0.7507177, \"raw_content\": null}, {\"title\": \"Cycles Ing\\u00e9nieurs avec Pr\\u00e9paratoires Int\\u00e9gr\\u00e9s - ISTMT\", \"url\": \"https://istmt.rnu.tn/cycles-ingenieurs-avec-preparatoires-integres/\", \"content\": \"Un Cycle Pr\\u00e9paratoire Int\\u00e9gr\\u00e9 qui dure 2 ans au cours duquel un seul redoublement est permis. Un Cycle de Formation pour l'obtention d'un Dipl\\u00f4me National d'Ing\\u00e9nieurs en G\\u00e9nie des Biotechnologies de la sant\\u00e9. Ce cycle dure 3 ans et l'admission \\u00e0 ce cycle se fait par voie de concours sur dossier ouvert aux \\u00e9tudiants :\", \"score\": 0.47504577, \"raw_content\": null}], \"response_time\": 1.75}\n",
            "Assistant: The integrated preparatory cycle in Tunisia is a two-year program that prepares students for engineering studies. It allows students to sit for examinations to enter engineering schools in Tunisia or abroad. After the preparatory cycle, students can continue their studies at universities in engineering specialties of their choice. One such program is offered at the University of Africa Sousse (UAS), which offers four specializations: Math-Physics. Another program is offered at the Higher Institute of Medical Technologies of Tunis (ISTMT). It is a two-year program, and acceptance into the following 3-year engineering program is based on a competitive application.\n",
            "User: What do you know about LangGraph?\n",
            "Assistant: \n",
            "Assistant: {\"query\": \"LangGraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.ibm.com/think/topics/langgraph\", \"title\": \"What is LangGraph? - IBM\", \"content\": \"LangGraph, created by [LangChain](https://www.ibm.com/think/topics/langchain), is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It provides a set of tools and libraries that enable users to create, run and optimize [large language models](https://www.ibm.com/think/topics/large-language-models) (LLMs) in a scalable and efficient manner. At its core, LangGraph uses the power of graph-based architectures to model and manage the [...] By using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without slowing down or sacrificing efficiency. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback. In the world of LLMs, this process is referred to as reflection. [...] **Agent systems**: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles or video games.\\n\\n**LLM applications**: By using LangGraph\\u2019s capabilities, developers can build more sophisticated AI models that learn and improve over time. Norwegian Cruise Line uses LangGraph to compile, construct and refine guest-facing AI solutions. This capability allows for improved and personalized guest experiences.\", \"score\": 0.9311691, \"raw_content\": null}, {\"url\": \"https://www.langchain.com/langgraph\", \"title\": \"LangGraph - LangChain\", \"content\": \"LangGraph is a stateful, orchestration framework that brings added control to agent workflows. LangGraph Platform is a service for deploying and scaling LangGraph applications, with an opinionated API for building agent UXs, plus an integrated developer studio.\\n\\nLangGraph (open source)\\n\\nLangGraph Platform\\n\\nFeatures\\n\\nDescription\\n\\nStateful orchestration framework for agentic applications\\n\\nScalable infrastructure for deploying LangGraph applications\\n\\nSDKs\\n\\nPython and JavaScript\", \"score\": 0.9224337, \"raw_content\": null}], \"response_time\": 1.3}\n",
            "Assistant: LangGraph is an open-source AI agent framework created by LangChain, designed for building, deploying, and managing complex generative AI agent workflows. It uses graph-based architectures to model and manage large language models (LLMs) in a scalable and efficient manner. LangGraph enables users to scale AI workflows without sacrificing efficiency and enhances decision-making by modeling complex relationships between nodes, using AI agents to analyze past actions and feedback, a process referred to as reflection in the world of LLMs. It provides a framework for building agent-based systems, useful in applications like robotics, autonomous vehicles, and video games, and allows developers to build more sophisticated AI models that learn and improve over time. There is also a LangGraph Platform, which is a service for deploying and scaling LangGraph applications, with an API for building agent UXs, plus an integrated developer studio.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Use prebuilts\n",
        "For ease of use, we adjust our code to replace the following with LangGraph prebuilt components. These have built in functionality like parallel API execution.\n",
        "\n",
        "BasicToolNode is replaced with the prebuilt ToolNode\n",
        "\n",
        "route_tools is replaced with the prebuilt tools_condition"
      ],
      "metadata": {
        "id": "u5dltmbrx6st"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_core.messages import BaseMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "tool = TavilySearch(max_results=2)\n",
        "tools = [tool]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "t0hOE9TwyFvZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> We've created a conversational agent in LangGraph that can use a search engine to retrieve updated information when needed. Now it can handle a wider range of user queries."
      ],
      "metadata": {
        "id": "wRTWyleZyY4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add memory\n",
        "The chatbot can now use tools to answer user questions, but it does not remember the context of previous interactions. This limits its ability to have coherent, multi-turn conversations.\n",
        "\n",
        "LangGraph solves this problem through persistent checkpointing. If you provide a checkpointer when compiling the graph and a thread_id when calling your graph, LangGraph automatically saves the state after each step. When you invoke the graph again using the same thread_id, the graph loads its saved state, allowing the chatbot to pick up where it left off.\n",
        "\n",
        "We will see later that checkpointing is much more powerful than simple chat memory - it lets you save and resume complex state at any time for error recovery, human-in-the-loop workflows, time travel interactions, and more. But first, let's add checkpointing to enable multi-turn conversations.\n",
        "\n",
        "\n",
        "\n",
        "# 1. Create a MemorySaver checkpointer"
      ],
      "metadata": {
        "id": "O8fvfMlj4Yrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "memory = MemorySaver()"
      ],
      "metadata": {
        "id": "Kpq2t3rZyLAO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in a production application, we would likely change this to use SqliteSaver or PostgresSaver and connect a database.\n",
        "\n",
        "# 2. Compile the graph¶\n",
        "Compile the graph with the provided checkpointer, which will checkpoint the State as the graph works through each node:"
      ],
      "metadata": {
        "id": "6NduQ6de6fFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = graph_builder.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "YxxhekKM5ofj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "GX3Ln4Df6-Mi",
        "outputId": "5140bb32-358c-484d-bdf9-91fbaf9c115d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Interact with chatbot"
      ],
      "metadata": {
        "id": "aDrDUfkF7Dju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a thread to use as the key for this conversation.\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Call the chatbot\n",
        "user_input = \"Hi there! My name is Will.\"\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsB7ybt96-rn",
        "outputId": "6de7c23c-626b-4ddb-c709-1aa2419144ea"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi there! My name is Will.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Will, nice to meet you! Is there anything I can help you with today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask a follow up question:\n",
        "user_input = \"Remember my name?\"\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr8CyOjJ7Qs7",
        "outputId": "3b93284a-7c75-4636-c13c-0486e98a6d1f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Remember my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Yes, your name is Will.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we aren't using an external list for memory: it's all handled by the checkpointer"
      ],
      "metadata": {
        "id": "3JxRDc1a7gWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The only difference is we change the `thread_id` here to \"2\" instead of \"1\"\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    {\"configurable\": {\"thread_id\": \"2\"}},\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypcgAFG97bnD",
        "outputId": "b359f527-c8ec-4083-ef5e-49e6ab20333b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Remember my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I am a stateless AI, I don't have memory of previous interactions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To inspect a graph's state for a given config at any time, call get_state(config)\n",
        "snapshot = graph.get_state(config)\n",
        "snapshot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3gR--jV7ldb",
        "outputId": "03d03658-6ee5-4cf0-f623-ae12a6ac7528"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content='Hi there! My name is Will.', additional_kwargs={}, response_metadata={}, id='869ebe1d-fac8-4b6d-b4a2-b5546b98d0ad'), AIMessage(content='Hi Will, nice to meet you! Is there anything I can help you with today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--8158780d-788a-48ec-b6bd-7b3e71806907-0', usage_metadata={'input_tokens': 491, 'output_tokens': 19, 'total_tokens': 510, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='Remember my name?', additional_kwargs={}, response_metadata={}, id='7b51845d-9a34-4d2e-a1a4-66c44808c171'), AIMessage(content='Yes, your name is Will.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--832be02a-7eff-4ed3-81ad-d74f4abfc513-0', usage_metadata={'input_tokens': 513, 'output_tokens': 8, 'total_tokens': 521, 'input_token_details': {'cache_read': 0}})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f050341-a1e0-6f47-8004-2576d5130b94'}}, metadata={'source': 'loop', 'writes': {'chatbot': {'messages': [AIMessage(content='Yes, your name is Will.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--832be02a-7eff-4ed3-81ad-d74f4abfc513-0', usage_metadata={'input_tokens': 513, 'output_tokens': 8, 'total_tokens': 521, 'input_token_details': {'cache_read': 0}})]}}, 'step': 4, 'parents': {}, 'thread_id': '1'}, created_at='2025-06-23T13:15:15.518416+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f050341-9c37-65bc-8003-1ea0139d1a2c'}}, tasks=(), interrupts=())"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The snapshot above contains the current state values, corresponding config, and the next node to process."
      ],
      "metadata": {
        "id": "Hj0kj-B-76Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snapshot.next  # (since the graph ended this turn, `next` is empty. If you fetch a state from within a graph invocation, next tells which node will execute next)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xqmAZ9Z7vqD",
        "outputId": "22642ec4-cc58-4438-d8c8-3d1aff486c8e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "()"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add human-in-the-loop controls\n",
        "Agents can be unreliable and may need human input to successfully accomplish tasks. Similarly, for some actions, you may want to require human approval before running to ensure that everything is running as intended.\n",
        "\n",
        "LangGraph's persistence layer supports human-in-the-loop workflows, allowing execution to pause and resume based on user feedback. The primary interface to this functionality is the interrupt function. Calling interrupt inside a node will pause execution. Execution can be resumed, together with new input from a human, by passing in a Command. interrupt is ergonomically similar to Python's built-in input(), with some caveats.\n",
        "\n",
        "# 1. Add the human_assistance tool\n",
        "add the human_assistance tool to the chatbot. This tool uses interrupt to receive information from a human.\n"
      ],
      "metadata": {
        "id": "jpmexDkF-ILf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_core.tools import tool\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "from langgraph.types import Command, interrupt\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "@tool\n",
        "def human_assistance(query: str) -> str:\n",
        "    \"\"\"Request assistance from a human.\"\"\"\n",
        "    human_response = interrupt({\"query\": query})\n",
        "    return human_response[\"data\"]\n",
        "\n",
        "tool = TavilySearch(max_results=2)\n",
        "tools = [tool, human_assistance]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    message = llm_with_tools.invoke(state[\"messages\"])\n",
        "    # Because we will be interrupting during tool execution,\n",
        "    # we disable parallel tool calling to avoid repeating any\n",
        "    # tool invocations when we resume.\n",
        "    assert len(message.tool_calls) <= 1\n",
        "    return {\"messages\": [message]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taTfE9s270fD",
        "outputId": "bad408f2-7258-4c6d-a309-d7e437e32307"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7ccbe27f6c90>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Compile the graph\n",
        "We compile the graph with a checkpointer, as before"
      ],
      "metadata": {
        "id": "2ZHCNOowBKrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = MemorySaver()\n",
        "\n",
        "graph = graph_builder.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "nIrEr5y5AuH0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Prompt the chatbot\n",
        "Now, prompt the chatbot with a question that will engage the new human_assistance tool"
      ],
      "metadata": {
        "id": "wwTuakY9BbRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"I need some expert guidance for building an AI agent. Could you request assistance for me?\"\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9wHasFDECSj",
        "outputId": "ca07aa22-54b9-4170-cab1-e8158ba71f80"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "I need some expert guidance for building an AI agent. Could you request assistance for me?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  human_assistance (1185fb63-bae6-42c6-9047-1b0d9bae6a26)\n",
            " Call ID: 1185fb63-bae6-42c6-9047-1b0d9bae6a26\n",
            "  Args:\n",
            "    query: I need some expert guidance for building an AI agent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chatbot generated a tool call, but then execution has been interrupted. If you inspect the graph state, you see that it stopped at the tools node"
      ],
      "metadata": {
        "id": "GhZPBVZfELgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snapshot = graph.get_state(config)\n",
        "snapshot.next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy3Wsz6tEEI3",
        "outputId": "cde3e0e8-deba-437a-9000-f816a9abab7c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tools',)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Resume execution\n",
        "To resume execution, we pass a Command object containing data expected by the tool. The format of this data can be customized based on needs. For this example, we use a dict with a key \"data\""
      ],
      "metadata": {
        "id": "o_7zJCQgEpPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_response = (\n",
        "    \"We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\"\n",
        "    \" It's much more reliable and extensible than simple autonomous agents.\"\n",
        ")\n",
        "\n",
        "human_command = Command(resume={\"data\": human_response})\n",
        "\n",
        "events = graph.stream(human_command, config, stream_mode=\"values\")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWRKFkIhEN6_",
        "outputId": "bae58194-2fd1-42fc-9dd4-05d367aaa10f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  human_assistance (1185fb63-bae6-42c6-9047-1b0d9bae6a26)\n",
            " Call ID: 1185fb63-bae6-42c6-9047-1b0d9bae6a26\n",
            "  Args:\n",
            "    query: I need some expert guidance for building an AI agent.\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: human_assistance\n",
            "\n",
            "We, the experts are here to help! We'd recommend you check out LangGraph to build your agent. It's much more reliable and extensible than simple autonomous agents.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The experts recommend checking out LangGraph to build your agent. They say it's more reliable and extensible than simple autonomous agents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customize state\n",
        "We will add additional fields to the state to define complex behavior without relying on the message list. The chatbot will use its search tool to find specific information and forward them to a human for review.\n",
        "\n",
        "# 1. Add keys to the state\n",
        "Update the chatbot to research the birthday of an entity by adding name and birthday keys to the state:"
      ],
      "metadata": {
        "id": "cvapV_3LJhyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    name: str\n",
        "    birthday: str"
      ],
      "metadata": {
        "id": "gphQEb4SFnxj"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding this information to the state makes it easily accessible by other graph nodes (like a downstream node that stores or processes the information), as well as the graph's persistence layer.\n",
        "\n",
        "# 2. Update the state inside the tool\n",
        "Now, populate the state keys inside of the human_assistance tool. This allows a human to review the information before it is stored in the state. Use Command to issue a state update from inside the tool."
      ],
      "metadata": {
        "id": "QvWb7_djKrtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "from langchain_core.tools import InjectedToolCallId, tool\n",
        "\n",
        "from langgraph.types import Command, interrupt\n",
        "\n",
        "@tool\n",
        "# Note that because we are generating a ToolMessage for a state update, we\n",
        "# generally require the ID of the corresponding tool call. We can use\n",
        "# LangChain's InjectedToolCallId to signal that this argument should not\n",
        "# be revealed to the model in the tool's schema.\n",
        "def human_assistance(\n",
        "    name: str, birthday: str, tool_call_id: Annotated[str, InjectedToolCallId]\n",
        ") -> str:\n",
        "    \"\"\"Request assistance from a human.\"\"\"\n",
        "    human_response = interrupt(\n",
        "        {\n",
        "            \"question\": \"Is this correct?\",\n",
        "            \"name\": name,\n",
        "            \"birthday\": birthday,\n",
        "        },\n",
        "    )\n",
        "    # If the information is correct, update the state as-is.\n",
        "    if human_response.get(\"correct\", \"\").lower().startswith(\"y\"):\n",
        "        verified_name = name\n",
        "        verified_birthday = birthday\n",
        "        response = \"Correct\"\n",
        "    # Otherwise, receive information from the human reviewer.\n",
        "    else:\n",
        "        verified_name = human_response.get(\"name\", name)\n",
        "        verified_birthday = human_response.get(\"birthday\", birthday)\n",
        "        response = f\"Made a correction: {human_response}\"\n",
        "\n",
        "    # This time we explicitly update the state with a ToolMessage inside\n",
        "    # the tool.\n",
        "    state_update = {\n",
        "        \"name\": verified_name,\n",
        "        \"birthday\": verified_birthday,\n",
        "        \"messages\": [ToolMessage(response, tool_call_id=tool_call_id)],\n",
        "    }\n",
        "    # We return a Command object in the tool to update our state.\n",
        "    return Command(update=state_update)"
      ],
      "metadata": {
        "id": "12E4cpXOKVFf"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Prompt the chatbot\n",
        "Prompt the chatbot to look up the \"birthday\" of the LangGraph library and direct the chatbot to reach out to the human_assistance tool once it has the required information. By setting name and birthday in the arguments for the tool, we force the chatbot to generate proposals for these fields."
      ],
      "metadata": {
        "id": "FZKQVtI9NrUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = (\n",
        "    \"Can you look up when LangGraph was released? \"\n",
        "    \"When you have the answer, use the human_assistance tool for review.\"\n",
        ")\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXDXkF0lNxYg",
        "outputId": "cb25a6e2-fd2d-4198-8747-808e3dd0610b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Can you look up when LangGraph was released? When you have the answer, use the human_assistance tool for review.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search (ac56f598-88a9-4b54-bf40-02726439ebc2)\n",
            " Call ID: ac56f598-88a9-4b54-bf40-02726439ebc2\n",
            "  Args:\n",
            "    query: When was LangGraph released?\n",
            "    topic: general\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search\n",
            "\n",
            "{\"query\": \"When was LangGraph released?\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Langgraph — Part 1 of LLM Multi-Agents series - Medium\", \"url\": \"https://medium.com/mitb-for-all/a-gentle-introduction-to-the-llm-multi-agents-multiverse-part-1-langgraph-2ac56f1b5b3c\", \"content\": \"Langgraph — Part 1 of LLM Multi-Agents series | by Tituslhy | MITB For All | Medium Langgraph — Part 1 of LLM Multi-Agents series Langgraph was released in January 2024 (“this year” at the time of writing) by the creators of LangChain, aiming to empower developers to build stateful, reliable multi-agent applications extending the capabilities of the LangChain framework. Just as the ecosystem of LLM orchestration libraries often takes LangChain as a reference point, the multi-agent systems landscape frequently looks to Langgraph, building their frameworks on similar core principles. code_node = functools.partial(agent_node, agent=code_agent, name=\\\"Coder\\\")from langgraph.graph import END, StateGraph, START However, Langgraph’s steep learning curve means developers should be prepared to write more code and spend additional time debugging Langgraph objects, especially with intricate multi-agent systems.\", \"score\": 0.85797083, \"raw_content\": null}, {\"title\": \"Releases · langchain-ai/langgraph - GitHub\", \"url\": \"https://github.com/langchain-ai/langgraph/releases\", \"content\": \"Releases · langchain-ai/langgraph · GitHub *   fix(langgraph): remove deprecated `output` usage in favor of `output_schema` (#5095) *   Remove Checkpoint.writes (#4822) *   Remove old checkpoint test fixtures (#4814) *   fix(langgraph): remove deprecated `output` usage in favor of `output_schema` (#5095) *   Remove support for node reading a single managed value *   Remove Checkpoint.writes (#4822) *   Remove Checkpoint.pending_sends (#4820) *   Remove old checkpoint test fixtures (#4814) Changes since checkpoint==2.0.26 *   langgraph-checkpoint 2.1.0 *   Preparation for 0.5 release: langgraph-checkpoint (#5124) *   Remove Checkpoint.writes *   Remove Checkpoint.pending_sends *   Remove Checkpoint.writes (#4822) *   Remove Checkpoint.pending_sends (#4820) *   Remove old checkpoint test fixtures (#4814) *   Remove postgres shallow checkpointer (#4813) *   Remove Checkpoint.writes *   Remove Checkpoint.pending_sends *   Remove old checkpoint test fixtures *   Remove postgres shallow checkpointer\", \"score\": 0.26367685, \"raw_content\": null}], \"response_time\": 1.67}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "LangGraph was released in January 2024.\n",
            "Tool Calls:\n",
            "  human_assistance (7120ca50-51f2-4401-94f2-811ee6a5e874)\n",
            " Call ID: 7120ca50-51f2-4401-94f2-811ee6a5e874\n",
            "  Args:\n",
            "    query: LangGraph was released in January 2024. Is this correct?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Add human assistance\n",
        "The chatbot failed to identify the correct date, so we supply it with information:"
      ],
      "metadata": {
        "id": "lkCAq1sjODpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_command = Command(\n",
        "    resume={\n",
        "        \"name\": \"LangGraph\",\n",
        "        \"birthday\": \"Jan 17, 2024\",\n",
        "    },\n",
        ")\n",
        "\n",
        "events = graph.stream(human_command, config, stream_mode=\"values\")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "444InnMUN4U0",
        "outputId": "4da00bd9-181c-4d85-acb8-3e29fcdc193c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "LangGraph was released in January 2024.\n",
            "Tool Calls:\n",
            "  human_assistance (7120ca50-51f2-4401-94f2-811ee6a5e874)\n",
            " Call ID: 7120ca50-51f2-4401-94f2-811ee6a5e874\n",
            "  Args:\n",
            "    query: LangGraph was released in January 2024. Is this correct?\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: human_assistance\n",
            "\n",
            "Error: KeyError('data')\n",
            " Please fix your mistakes.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I apologize, it seems I encountered an error. I will try a different approach to confirm the release date.\n",
            "Tool Calls:\n",
            "  tavily_search (629eda45-2256-491d-af21-030a915f36a9)\n",
            " Call ID: 629eda45-2256-491d-af21-030a915f36a9\n",
            "  Args:\n",
            "    query: official announcement of LangGraph release date\n",
            "    topic: general\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search\n",
            "\n",
            "{\"query\": \"official announcement of LangGraph release date\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Changelog | Introducing LangGraph - LangChain\", \"url\": \"https://changelog.langchain.com/announcements/week-of-1-22-24-langchain-release-notes\", \"content\": \"LangChain - Changelog | 🤖🚀 Introducing LangGraph Sign up LangChain Changelog Sign up for our newsletter to stay up to date **DATE:**January 22, 2024 LangGraph 🤖🚀 Introducing LangGraph **DATE:**January 22, 2024 Introducing LangGraph We just launched LangGraph, which helps customize your Agent Runtime. LangGraph helps construct a powerful agent executor that allows for loops in logic while keeping track of application state. Agents constructed with LangGraph can handle more ambiguous inputs and accomplish tasks more consistently. These agents can: *   Manage agent steps We’re excited to give our agents more _controllable_ power. Learn about LangGraph by reading our docs. ##### Subscribe to updates Subscribe By clicking subscribe, you accept our privacy policy and terms and conditions. reCAPTCHA privacy and terms apply\", \"score\": 0.7355091, \"raw_content\": null}, {\"title\": \"Customize state - langchain-ai.lang.chat\", \"url\": \"https://langchain-ai.lang.chat/langgraph/tutorials/get-started/5-customize-state/\", \"content\": \"Prompt the chatbot to look up the \\\"birthday\\\" of the LangGraph library and direct the chatbot to reach out to the `human_assistance` tool once it has the required information. Then, I'll use the human_assistance tool for review.\\\", 'type': 'text'}, {'id': 'toolu_01JoXQPgTVJXiuma8xMVwqAi', 'input': {'query': 'LangGraph release date'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}] \\\\n\\\\nGiven this information, I'll use the human_assistance tool to review and potentially provide more accurate information about LangGraph's initial release date.\\\", 'type': 'text'}, {'id': 'toolu_01JDQAV7nPqMkHHhNs3j3XoN', 'input': {'name': 'Assistant', 'birthday': '2023-01-01'}, 'name': 'human_assistance', 'type': 'tool_use'}] \\\\n\\\\nGiven this information, I'll use the human_assistance tool to review and potentially provide more accurate information about LangGraph's initial release date.\\\", 'type': 'text'}, {'id': 'toolu_01JDQAV7nPqMkHHhNs3j3XoN', 'input': {'name': 'Assistant', 'birthday': '2023-01-01'}, 'name': 'human_assistance', 'type': 'tool_use'}]\", \"score\": 0.5634506, \"raw_content\": null}], \"response_time\": 1.79}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  human_assistance (ae3e699c-5794-47ec-8341-a2c48214b6e4)\n",
            " Call ID: ae3e699c-5794-47ec-8341-a2c48214b6e4\n",
            "  Args:\n",
            "    query: LangGraph was released on January 22, 2024. Is this correct?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time travel\n",
        "In a typical chatbot workflow, the user interacts with the bot one or more times to accomplish a task. Memory and a human-in-the-loop enable checkpoints in the graph state and control future responses.\n",
        "\n",
        "What if we want a user to be able to start from a previous response and explore a different outcome? Or what if we want users to be able to rewind the chatbot's work to fix mistakes or try a different strategy, something that is common in applications like autonomous software engineers?\n",
        "\n",
        "we can create these types of experiences using LangGraph's built-in time travel functionality."
      ],
      "metadata": {
        "id": "Pd50c70zPDMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Rewind yhe graph\n",
        "Rewind the graph by fetching a checkpoint using the graph's get_state_history method. we can then resume execution at this previous point in time."
      ],
      "metadata": {
        "id": "sHlK9pIKQmHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_core.messages import BaseMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "tool = TavilySearch(max_results=2)\n",
        "tools = [tool]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "memory = MemorySaver()\n",
        "graph = graph_builder.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "n6_HtSFuOoPI"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Add steps\n",
        "Every step will be checkpointed in its state history"
      ],
      "metadata": {
        "id": "ZpdJ2MN-Ra4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "events = graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"I'm learning LangGraph. \"\n",
        "                    \"Could you do some research on it for me?\"\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9aI2fD5QuZw",
        "outputId": "56ea96a6-31de-4af7-9c08-afd332548d2c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "I'm learning LangGraph. Could you do some research on it for me?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search (390170a7-6f2a-4055-bffe-dd9c450489f4)\n",
            " Call ID: 390170a7-6f2a-4055-bffe-dd9c450489f4\n",
            "  Args:\n",
            "    query: LangGraph\n",
            "    topic: general\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search\n",
            "\n",
            "{\"query\": \"LangGraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"What is LangGraph? - IBM\", \"url\": \"https://www.ibm.com/think/topics/langgraph\", \"content\": \"LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent’s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.\", \"score\": 0.942002, \"raw_content\": null}, {\"title\": \"LangGraph - LangChain\", \"url\": \"https://www.langchain.com/langgraph\", \"content\": \"Design agent-driven user experiences with LangGraph Platform's APIs. Quickly deploy and scale your application with infrastructure built for agents. LangGraph sets the foundation for how we can build and scale AI workloads — from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.” LangGraph sets the foundation for how we can build and scale AI workloads — from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. LangGraph Platform is a service for deploying and scaling LangGraph applications, with an opinionated API for building agent UXs, plus an integrated developer studio.\", \"score\": 0.81595254, \"raw_content\": null}], \"response_time\": 1.43}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "LangGraph, created by LangChain, is an open-source AI agent framework designed for building, deploying, and managing complex generative AI agent workflows. It uses graph-based architectures to model and manage the relationships between components in an AI agent workflow, providing transparency into the agent's state. LangGraph allows users to develop AI solutions and workflows, including chatbots, state graphs, and other agent-based systems. In LangGraph, nodes represent individual components or agents within an AI workflow, using AI agents to analyze their past actions and feedback for enhanced decision-making by modeling complex relationships between nodes. LangGraph Platform is a service for deploying and scaling LangGraph applications, with an API for building agent UXs, plus an integrated developer studio.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "events = graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"Ya that's helpful. Maybe I'll \"\n",
        "                    \"build an autonomous agent with it!\"\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS-7x5VpR0Tj",
        "outputId": "9f9ef574-f12f-43b2-c53d-0f106cdc1b70"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Ya that's helpful. Maybe I'll build an autonomous agent with it!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "That sounds like a cool project! Let me know if you have any other questions about LangGraph or if there's anything else I can help you with along the way.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Replay the full state history\n",
        "Now that we have added steps to the chatbot, we can replay the full state history to see everything that occurred"
      ],
      "metadata": {
        "id": "aCJ0OSMWR_hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_replay = None\n",
        "for state in graph.get_state_history(config):\n",
        "    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n",
        "    print(\"-\" * 80)\n",
        "    if len(state.values[\"messages\"]) == 6:\n",
        "        # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.\n",
        "        to_replay = state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKjsVlN2R5J7",
        "outputId": "c1044149-8cdb-4ea4-88a1-92fd22e426d6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Messages:  6 Next:  ()\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  5 Next:  ('chatbot',)\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  4 Next:  ('__start__',)\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  4 Next:  ()\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  3 Next:  ('chatbot',)\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  2 Next:  ('tools',)\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  1 Next:  ('chatbot',)\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  0 Next:  ('__start__',)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume from a checkpoint\n",
        "Resume from the to_replay state, which is after the chatbot node in the second graph invocation. Resuming from this point will call the action node next."
      ],
      "metadata": {
        "id": "BOhqJ-vFSKs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(to_replay.next)\n",
        "print(to_replay.config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEJQt7s5SD67",
        "outputId": "9b6f21f2-3caf-4bc1-e41d-4bbb520d6ce0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "()\n",
            "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f05041c-fd82-6afb-8006-aa7b067006a9'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Load a state from a moment-in-time\n",
        "The checkpoint's to_replay.config contains a checkpoint_id timestamp. Providing this checkpoint_id value tells LangGraph's checkpointer to load the state from that moment in time."
      ],
      "metadata": {
        "id": "h3UU-Op5SgWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The `checkpoint_id` in the `to_replay.config` corresponds to a state we've persisted to our checkpointer.\n",
        "for event in graph.stream(None, to_replay.config, stream_mode=\"values\"):\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWLNRmBgSZGI",
        "outputId": "eed640b4-5143-4f31-8bce-3fd14f3c1070"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "That sounds like a cool project! Let me know if you have any other questions about LangGraph or if there's anything else I can help you with along the way.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O-s6fHCZSj5Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}